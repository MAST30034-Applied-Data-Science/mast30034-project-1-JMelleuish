{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAST30034 Project 1\n",
    "## Preprocessing taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/15 20:59:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce \n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading taxi data for yellow and green taxis from jan 2018 - dec 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin yellow taxi data\n",
      "Begin month 01/2018\n",
      "Completed month 01/2018\n",
      "Begin month 02/2018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ck/psrltvd10474n3pnlfszjw500000gn/T/ipykernel_44593/1330241978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{output_relative_dir}/{taxi}_tripdata_{year}-{month}.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completed month {month}/{year}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "# from the current `tute_1` directory, go back two levels to the `MAST30034` directory\n",
    "output_relative_dir = '../data/raw'\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "\n",
    "TAXI  = ['yellow', 'green']\n",
    "YEARS = ['2018', '2019']\n",
    "MONTHS = range(1, 13)\n",
    "for taxi in TAXI:\n",
    "    # this is the URL template as of 07/2022\n",
    "    URL_TEMPLATE = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi}_tripdata_\"#year-month.parquet\n",
    "\n",
    "    print(f\"Begin {taxi} taxi data\")\n",
    "    for year in YEARS:\n",
    "        for month in MONTHS:\n",
    "            # 0-fill i.e 1 -> 01, 2 -> 02, etc\n",
    "            month = str(month).zfill(2) \n",
    "            print(f\"Begin month {month}/{year}\")\n",
    "\n",
    "            # generate url\n",
    "            url = f'{URL_TEMPLATE}{year}-{month}.parquet'\n",
    "            # generate output location and filename\n",
    "            output_dir = f\"{output_relative_dir}/{taxi}_tripdata_{year}-{month}.parquet\"\n",
    "            # download\n",
    "            urlretrieve(url, output_dir) \n",
    "\n",
    "            print(f\"Completed month {month}/{year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Green taxi service has an extra column called ehail_fee, to help combine the dataframes, all yellow taxi dataframes will have a ehail_fee column appended to it with 0 for each row as yellow taxis despite having an ehail service, dont charge for it.\n",
    "\n",
    "Green taxi services also has trip_type whereas yellow taxis do not, given that yellow taxis can both hail from the street as well as being pre organised, it is impossible to populate this feild with any real meaningful data thus it will be removd from the green taxi dataframes.\n",
    "\n",
    "Yellow taxis also have an airport_fee column, dispite Green taxis being able to go the airport, they do log any infomation of airport_fees, similar to the trip_type situation, it is unreasonable to populate this column with meaningful data and thus wil be removed from all yellow taxi dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "Gjan19 = spark.read.parquet('../data/raw/green_tripdata_2019-01.parquet').drop(\"trip_type\")\n",
    "Gfeb19 = spark.read.parquet('../data/raw/green_tripdata_2019-02.parquet').drop(\"trip_type\")\n",
    "Gmar19 = spark.read.parquet('../data/raw/green_tripdata_2019-03.parquet').drop(\"trip_type\")\n",
    "Gapr19 = spark.read.parquet('../data/raw/green_tripdata_2019-04.parquet').drop(\"trip_type\")\n",
    "Gmay19 = spark.read.parquet('../data/raw/green_tripdata_2019-05.parquet').drop(\"trip_type\")\n",
    "Gjun19 = spark.read.parquet('../data/raw/green_tripdata_2019-06.parquet').drop(\"trip_type\")\n",
    "Gjul19 = spark.read.parquet('../data/raw/green_tripdata_2019-07.parquet').drop(\"trip_type\")\n",
    "Gaug19 = spark.read.parquet('../data/raw/green_tripdata_2019-08.parquet').drop(\"trip_type\")\n",
    "Gsep19 = spark.read.parquet('../data/raw/green_tripdata_2019-09.parquet').drop(\"trip_type\")\n",
    "Goct19 = spark.read.parquet('../data/raw/green_tripdata_2019-10.parquet').drop(\"trip_type\")\n",
    "Gnov19 = spark.read.parquet('../data/raw/green_tripdata_2019-11.parquet').drop(\"trip_type\")\n",
    "Gdec19 = spark.read.parquet('../data/raw/green_tripdata_2019-12.parquet').drop(\"trip_type\")\n",
    "\n",
    "Gjan18 = spark.read.parquet('../data/raw/green_tripdata_2018-01.parquet').drop(\"trip_type\")\n",
    "Gfeb18 = spark.read.parquet('../data/raw/green_tripdata_2018-02.parquet').drop(\"trip_type\")\n",
    "Gmar18 = spark.read.parquet('../data/raw/green_tripdata_2018-03.parquet').drop(\"trip_type\")\n",
    "Gapr18 = spark.read.parquet('../data/raw/green_tripdata_2018-04.parquet').drop(\"trip_type\")\n",
    "Gmay18 = spark.read.parquet('../data/raw/green_tripdata_2018-05.parquet').drop(\"trip_type\")\n",
    "Gjun18 = spark.read.parquet('../data/raw/green_tripdata_2018-06.parquet').drop(\"trip_type\")\n",
    "Gjul18 = spark.read.parquet('../data/raw/green_tripdata_2018-07.parquet').drop(\"trip_type\")\n",
    "Gaug18 = spark.read.parquet('../data/raw/green_tripdata_2018-08.parquet').drop(\"trip_type\")\n",
    "Gsep18 = spark.read.parquet('../data/raw/green_tripdata_2018-09.parquet').drop(\"trip_type\")\n",
    "Goct18 = spark.read.parquet('../data/raw/green_tripdata_2018-10.parquet').drop(\"trip_type\")\n",
    "Gnov18 = spark.read.parquet('../data/raw/green_tripdata_2018-11.parquet').drop(\"trip_type\")\n",
    "Gdec18 = spark.read.parquet('../data/raw/green_tripdata_2018-12.parquet').drop(\"trip_type\")\n",
    "\n",
    "Yjan19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-01.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yfeb19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-02.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymar19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-03.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yapr19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-04.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymay19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-05.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjun19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-06.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjul19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-07.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yaug19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-08.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ysep19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-09.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yoct19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-10.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ynov19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-11.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ydec19 = spark.read.parquet('../data/raw/yellow_tripdata_2019-12.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "\n",
    "Yjan18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-01.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yfeb18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-02.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymar18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-03.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yapr18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-04.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymay18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-05.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjun18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-06.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjul18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-07.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yaug18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-08.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ysep18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-09.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yoct18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-10.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ynov18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-11.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ydec18 = spark.read.parquet('../data/raw/yellow_tripdata_2019-12.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "184397591"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = unionAll(*[ \\\n",
    "     Gjan19, Gfeb19, Gmar19, Gapr19, Gmay19, Gjun19, Gjul19, Gaug19, Gsep19, Goct19, Gnov19, Gdec19,\\\n",
    "     Gjan18, Gfeb18, Gmar18, Gapr18, Gmay18, Gjun18, Gjul18, Gaug18, Gsep18, Goct18, Gnov18, Gdec18, \\\n",
    "     Yjan19, Yfeb19, Ymar19, Yapr19, Ymay19, Yjun19, Yjul19, Yaug19, Ysep19, Yoct19, Ynov19, Ydec19,\\\n",
    "     Yjan18, Yfeb18, Ymar18, Yapr18, Ymay18, Yjun18, Yjul18, Yaug18, Ysep18, Yoct18, Ynov18, Ydec18,])\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropped Columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VendorID is used to indicate the provider who indicated the record, this inforation is not nesscary to know for this project as it has no effect of the taxis themselves. \n",
    "\n",
    "RatecodeID is used to indicate the kind of rate used (to calculate payment) during the ride. This is unescary information due to other columns.\n",
    "\n",
    "store_and_fwd_flag is used to flag if the vechile had a conection to the service during payment, this information is unnessacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.drop(\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improvement_surcharge, mta_tax, extra, tolls_amount, ehail_fee and congestion_surcharge are all various surcharges applied to the total fee to the customer. These on their own dont hold alot of information but the total will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.na.fill(value=0,subset=[\"congestion_surcharge\", \"ehail_fee\"])\n",
    "sdf = sdf.withColumn('surcharge_amount', col(\"improvement_surcharge\") + col(\"mta_tax\") + col(\"extra\") + col(\"tolls_amount\") + col(\"ehail_fee\") + col(\"congestion_surcharge\"))\n",
    "sdf = sdf.drop(\"extra\", \"mta_tax\", \"improvement_surcharge\", \"congestion_surcharge\", \"tolls_amount\", \"ehail_fee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_time\")\n",
    "sdf = sdf.withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_time\")\n",
    "sdf = sdf.withColumnRenamed(\"PULocationID\", \"pickup_location\")\n",
    "sdf = sdf.withColumnRenamed(\"DOLocationID\", \"dropoff_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trip_distance is set to miles (imperial), thus for ease of analysis, this will be set to kilometres (metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"trip_distance_km\", round(col(\"trip_distance\") * 1.60934, 3)) \n",
    "sdf = sdf.drop('trip_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Trip duration (in minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are time stamps of the pickup an dropoff times, it may be handy to look into aspects affecting the duration of the trip, thus we will add a column for total duration in minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.withColumn('trip_time_sec',unix_timestamp(\"dropoff_time\") - unix_timestamp('pickup_time'))\n",
    "sdf = sdf.withColumn('trip_time_min', round(col('trip_time_sec') / 60, 3))\n",
    "sdf = sdf.drop('trip_time_sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- dropoff_time: timestamp (nullable = true)\n",
      " |-- pickup_location: double (nullable = true)\n",
      " |-- dropoff_location: string (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- surcharge_amount: double (nullable = true)\n",
      " |-- trip_distance_km: double (nullable = true)\n",
      " |-- trip_time_min: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('passenger_count', sdf.passenger_count.cast('int'))\n",
    "sdf = sdf.withColumn('pickup_location', sdf.pickup_location.cast('long'))\n",
    "sdf = sdf.withColumn('dropoff_location', sdf.pickup_location.cast('long'))\n",
    "sdf = sdf.withColumn('payment_type', sdf.payment_type.cast('int'))\n",
    "sdf = sdf.withColumn('payment_type', sdf.payment_type.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalidating data\n",
    "#### passenger_count:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the reasonable minimum nuber of passengers is 1, For taxis in NYC the maximum legal number of passengers is 5, thus we will onyl keep instances between 1 and 5 passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.passenger_count > 0)\n",
    "sdf = sdf.filter(sdf.passenger_count < 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickup/dropoff time:\n",
    "ensuring all data comes from the two year window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.pickup_time >= '2018-01-01 00:00:00')\n",
    "sdf = sdf.filter(sdf.pickup_time < '2020-01-01 00:00:00')\n",
    "sdf = sdf.filter(sdf.dropoff_time >= '2018-01-01 00:00:00')\n",
    "sdf = sdf.filter(sdf.dropoff_time < '2020-01-01 00:00:00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip_distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As New York City from north to south (Tottenville to Wakefield) is approximately 75km long trip according to https://www.taxi-calculator.com/taxi-fare-estimation, we will set that as the upper bound of acceptable taxi trip distances, and of course all non-negative values as the lower bound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.trip_distance_km > 0)\n",
    "sdf = sdf.filter(sdf.trip_distance_km < 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total_amount:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://www.taxifarefinder.com/main.php?city=NY&from=Conference+House+Park%2C+7455+Hylan+Blvd%2C+New+York+City%2C+New+York%2C+10307%2C+United+States+of+America&to=Metro+North+-+Wakefield+Station%2C+E+241st+St%2FWakefield+Ave%2C+New+York+City%2C+New+York%2C+10470%2C+United+States+of+America&fromCoord=40.502859,-74.252418&toCoord=40.905372,-73.85469 the longest trip would cost $129.72 to travel excluding tips, thus assuming a very generous tip of $100, we will set our upper most boundary at $230."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.total_amount > 0)\n",
    "sdf = sdf.filter(sdf.total_amount < 230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fare_amount:\n",
    "Although some taxi rides may be given as free, these are clearly outliers and wont follow tipical trends and thus should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.fare_amount > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip_time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the reasonable maximum travel diatance of 75km, we assume an average travel speed of 40km/h (condering extremely bad traffic as well as highways) which works out to be approximately 2 hours, thus we can set this as the reasonable upper bound and 0 as the reasonable lower bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.trip_time_min > 0)\n",
    "sdf = sdf.filter(sdf.trip_time_min < 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payment Type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data dictionary, only payment types of 1 (credit card) has records of tips thus we will only keep payments type 1 and then we wil drop the column from thw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.payment_type == 1)\n",
    "sdf = sdf.drop('payment_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "import pyspark.sql.functions as F\n",
    "sdf = sdf.withColumn('pickup_hour_time', date_format('pickup_time', 'HH'))\n",
    "sdf = sdf.withColumn('pickup_hour_time', F.regexp_replace('pickup_hour_time', r'^(0){1}', ''))\n",
    "sdf = sdf.withColumn('pickup_hour_time', sdf.pickup_hour_time.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/16 08:48:21 WARN DAGScheduler: Broadcasting large task binary with size 1577.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf.write.parquet(\"../data/curated/taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7882213"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "863906e9a4165d4738736793daf13ef61aad6cfe5622e28d202ed76ed0afc263"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
