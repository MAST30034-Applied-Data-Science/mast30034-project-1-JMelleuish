{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAST30034 Project 1\n",
    "## Statistical Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols, glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/14 18:11:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/14 18:11:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce \n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = spark.read.parquet('../../Project 1/DataFrames/taxi')\n",
    "weather = spark.read.parquet('../../Project 1/DataFrames/weather')\n",
    "taxi = taxi.withColumn(\"pickup_date\",to_date(col(\"pickup_time\")))\n",
    "taxi = taxi.drop(\"pickup_time\", \"dropoff_time\")\n",
    "sdf = taxi.join(weather,taxi.pickup_date ==  weather.date,\"inner\")\n",
    "sdf = sdf.drop('date')\n",
    "sdf.write.parquet(\"../../Project 1/DataFrames/stats_modelling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary at 0x7f7d732b5f40>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "#eatures = 'features'\n",
    "#input_cols = ['fare_amount', 'passenger_count', 'pickup_location', 'trip_distance_km', 'temp', 'dew_point', 'pressure', 'wind_speed', 'wind_direction']\n",
    "\n",
    "formula=RFormula(formula = \"tip_amount ~ fare_amount + passenger_count + pickup_location + trip_distance_km + temp + dew_point + pressure + wind_speed + wind_direction\", featuresCol= \"features\", labelCol= \"label\")\n",
    "output = formula.fit(sdf).transform(sdf)\n",
    "model_sdf = output.select(\"label\",\"features\")\n",
    "\n",
    "model_sdf = model_sdf.withColumn(\"label\", when(model_sdf[\"label\"] > 0, 1).otherwise(model_sdf[\"label\"]))\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = model_sdf.randomSplit([0.8, 0.2], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Fit the model\n",
    "lrModel = LogisticRegression().fit(train)\n",
    "lrModel.summary\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "# print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "# print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_weather = ols(\n",
    "    formula=\"tip_amount ~ temp + dew_point + pressure + wind_speed + wind_direction\",\n",
    "    data=df\n",
    ").fit()\n",
    "\n",
    "fit_taxi = ols(\n",
    "    formula=\"tip_amount ~ pickup_location + passenger_count + fare_amount + trip_distance_km\",\n",
    "    data=df\n",
    ").fit()\n",
    "\n",
    "fit_all = ols(\n",
    "    formula=\"tip_amount ~ temp + dew_point + pressure + wind_speed + wind_direction + pickup_location + passenger_count + fare_amount + trip_distance_km\",\n",
    "    data=df\n",
    ").fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.220367188411906"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sdf[['tip_amount']] <= 10)\n",
    "(sdf.select('tip_amount').where(sdf.tip_amount == 0).count() / sdf.select('tip_amount').count())*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As over 34% of people dont give a tip, for simplicity, the percentron will be trained to classify an instance as either 0 (didnt tip) or 1 (tipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import RFormula\n",
    "sdf = spark.read.parquet(\"../../Project 1/DataFrames/stats_modelling\")\n",
    "#formula=RFormula(formula = \"tip_amount ~ fare_amount + passenger_count + pickup_location + trip_distance_km + temp + dew_point + pressure + wind_speed + wind_direction\", featuresCol= \"features\", labelCol= \"label\")\n",
    "formula=RFormula(formula = \"tip_amount ~ fare_amount + trip_distance_km + temp + trip_time_min\", featuresCol= \"features\", labelCol= \"label\")\n",
    "output = formula.fit(sdf).transform(sdf)\n",
    "model_sdf = output.select(\"label\",\"features\")\n",
    "model_sdf = model_sdf.withColumn(\"label\", when(model_sdf[\"label\"] > 0, 1).otherwise(model_sdf[\"label\"]))\n",
    "# Load training data\n",
    "model_sdf = model_sdf.withColumn(\"label\",col('label').cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into train and test\n",
    "splits = model_sdf.randomSplit([0.8, 0.2], 1)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 11, 9, 2]\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, seed = 1)\n",
    "model = trainer.fit(train)\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(evaluator.evaluate(predictionAndLabels))\n",
    "\n",
    "layers = [4, 11, 9, 7, 2]\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, seed = 1)\n",
    "model = trainer.fit(train)\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(evaluator.evaluate(predictionAndLabels))\n",
    "\n",
    "layers = [4, 11, 9, 7, 5, 2]\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, seed = 1)\n",
    "model = trainer.fit(train)\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(evaluator.evaluate(predictionAndLabels))\n",
    "\n",
    "layers = [4, 11, 9, 7, 5, 3, 2]\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, seed = 1)\n",
    "model = trainer.fit(train)\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(evaluator.evaluate(predictionAndLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that a smaller number of hidden layers is optimal for this neutral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(5, 12, 2):\n",
    "    for j in range(5, 12, 2):\n",
    "        layers = [4, i, j, 2]\n",
    "        trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, seed = 1)\n",
    "        model = trainer.fit(train)\n",
    "        result = model.transform(test)\n",
    "        predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "        evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "        outputs.append(evaluator.evaluate(predictionAndLabels))\n",
    "        print(evaluator.evaluate(predictionAndLabels))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.6601338272229761, 0.6863059061696939, 0.6762855892135433, 0.6739921146359164, 0.6912520742346965, 0.6683100568205782, 0.6584308096387014, 0.6810819615388531, 0.6840938739359775, 0.6918410769004789, 0.6939643951524354, 0.6945868473523239, 0.6719982315376751, 0.6832823591520106, 0.6888291645033544, 0.6851671676825145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(5, 12, 2):\n",
    "    for j in range(5, 12, 2):\n",
    "        layers = [4, 13, i, j, 2]\n",
    "        trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, seed = 1)\n",
    "        model = trainer.fit(train)\n",
    "        result = model.transform(test)\n",
    "        predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "        evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "        outputs.append(evaluator.evaluate(predictionAndLabels))\n",
    "        print(evaluator.evaluate(predictionAndLabels))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.6841971311934357, 0.6864775168229341, 0.6784278137239076, 0.6584308096387014, 0.6846799679466203, 0.6591521561133387, 0.6921828438793897, 0.6577356410603211, 0.680402790563741, 0.6810179711257804, 0.6584308096387014, 0.6854158576969559, 0.6876933346713147, 0.6615634312241221, 0.6798225138633776, 0.6833623471683515]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above outputs, it can been seen that the optimal neural network is: [4, 9, 11, 2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3302:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7223296000744616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "layers = [4, 9, 11, 2]\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, seed = 1)\n",
    "model = trainer.fit(train)\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(evaluator.evaluate(predictionAndLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 fold cross validation\n",
    "- think of a way to graph it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
