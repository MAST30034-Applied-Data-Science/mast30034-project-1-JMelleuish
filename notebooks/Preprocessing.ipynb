{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAST30034 Project 1\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential questions:\n",
    "How big of an impact do tourists have on taxi revenue?\n",
    "Do road accidents effecct the duration of taxi trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 15:23:40 WARN Utils: Your hostname, Jacks-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.12.232.82 instead (on interface en0)\n",
      "22/08/08 15:23:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 15:23:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/08 15:23:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce \n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Project 1 - Preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Green taxi service has an extra column called ehail_fee, to help combine the dataframes, all yellow taxi dataframes will have a ehail_fee column appended to it with 0 for each row as yellow taxis despite having an ehail service, dont charge for it.\n",
    "\n",
    "Green taxi services also has trip_type whereas yellow taxis do not, given that yellow taxis can both hail from the street as well as being pre organised, it is impossible to populate this feild with any real meaningful data thus it will be removd from the green taxi dataframes.\n",
    "\n",
    "Yellow taxis also have an airport_fee column, dispite Green taxis being able to go the airport, they do log any infomation of airport_fees, similar to the trip_type situation, it is unreasonable to populate this column with meaningful data and thus wil be removed from all yellow taxi dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "Gjan19 = spark.read.parquet('taxi_data/green_tripdata_2019-01.parquet').drop(\"trip_type\")\n",
    "Gfeb19 = spark.read.parquet('taxi_data/green_tripdata_2019-02.parquet').drop(\"trip_type\")\n",
    "Gmar19 = spark.read.parquet('taxi_data/green_tripdata_2019-03.parquet').drop(\"trip_type\")\n",
    "Gapr19 = spark.read.parquet('taxi_data/green_tripdata_2019-04.parquet').drop(\"trip_type\")\n",
    "Gmay19 = spark.read.parquet('taxi_data/green_tripdata_2019-05.parquet').drop(\"trip_type\")\n",
    "Gjun19 = spark.read.parquet('taxi_data/green_tripdata_2019-06.parquet').drop(\"trip_type\")\n",
    "Gjul19 = spark.read.parquet('taxi_data/green_tripdata_2019-07.parquet').drop(\"trip_type\")\n",
    "Gaug19 = spark.read.parquet('taxi_data/green_tripdata_2019-08.parquet').drop(\"trip_type\")\n",
    "Gsep19 = spark.read.parquet('taxi_data/green_tripdata_2019-09.parquet').drop(\"trip_type\")\n",
    "Goct19 = spark.read.parquet('taxi_data/green_tripdata_2019-10.parquet').drop(\"trip_type\")\n",
    "Gnov19 = spark.read.parquet('taxi_data/green_tripdata_2019-11.parquet').drop(\"trip_type\")\n",
    "Gdec19 = spark.read.parquet('taxi_data/green_tripdata_2019-12.parquet').drop(\"trip_type\")\n",
    "\n",
    "Gjan18 = spark.read.parquet('taxi_data/green_tripdata_2018-01.parquet').drop(\"trip_type\")\n",
    "Gfeb18 = spark.read.parquet('taxi_data/green_tripdata_2018-02.parquet').drop(\"trip_type\")\n",
    "Gmar18 = spark.read.parquet('taxi_data/green_tripdata_2018-03.parquet').drop(\"trip_type\")\n",
    "Gapr18 = spark.read.parquet('taxi_data/green_tripdata_2018-04.parquet').drop(\"trip_type\")\n",
    "Gmay18 = spark.read.parquet('taxi_data/green_tripdata_2018-05.parquet').drop(\"trip_type\")\n",
    "Gjun18 = spark.read.parquet('taxi_data/green_tripdata_2018-06.parquet').drop(\"trip_type\")\n",
    "Gjul18 = spark.read.parquet('taxi_data/green_tripdata_2018-07.parquet').drop(\"trip_type\")\n",
    "Gaug18 = spark.read.parquet('taxi_data/green_tripdata_2018-08.parquet').drop(\"trip_type\")\n",
    "Gsep18 = spark.read.parquet('taxi_data/green_tripdata_2018-09.parquet').drop(\"trip_type\")\n",
    "Goct18 = spark.read.parquet('taxi_data/green_tripdata_2018-10.parquet').drop(\"trip_type\")\n",
    "Gnov18 = spark.read.parquet('taxi_data/green_tripdata_2018-11.parquet').drop(\"trip_type\")\n",
    "Gdec18 = spark.read.parquet('taxi_data/green_tripdata_2018-12.parquet').drop(\"trip_type\")\n",
    "\n",
    "Yjan19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-01.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yfeb19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-02.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymar19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-03.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yapr19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-04.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymay19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-05.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjun19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-06.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjul19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-07.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yaug19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-08.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ysep19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-09.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yoct19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-10.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ynov19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-11.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ydec19 = spark.read.parquet('taxi_data/yellow_tripdata_2019-12.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "\n",
    "Yjan18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-01.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yfeb18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-02.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymar18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-03.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yapr18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-04.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ymay18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-05.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjun18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-06.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yjul18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-07.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yaug18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-08.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ysep18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-09.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Yoct18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-10.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ynov18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-11.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")\n",
    "Ydec18 = spark.read.parquet('taxi_data/yellow_tripdata_2019-12.parquet').withColumn('ehail_fee', lit(0)).drop(\"airport_fee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "184397591"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = unionAll(*[ \\\n",
    "     Gjan19, Gfeb19, Gmar19, Gapr19, Gmay19, Gjun19, Gjul19, Gaug19, Gsep19, Goct19, Gnov19, Gdec19,\\\n",
    "     Gjan18, Gfeb18, Gmar18, Gapr18, Gmay18, Gjun18, Gjul18, Gaug18, Gsep18, Goct18, Gnov18, Gdec18, \\\n",
    "     Yjan19, Yfeb19, Ymar19, Yapr19, Ymay19, Yjun19, Yjul19, Yaug19, Ysep19, Yoct19, Ynov19, Ydec19,\\\n",
    "     Yjan18, Yfeb18, Ymar18, Yapr18, Ymay18, Yjun18, Yjul18, Yaug18, Ysep18, Yoct18, Ynov18, Ydec18,])\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropped Columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VendorID is used to indicate the provider who indicated the record, this inforation is not nesscary to know for this project as it has no effect of the taxis themselves. \n",
    "\n",
    "RatecodeID is used to indicate the kind of rate used (to calculate payment) during the ride. This is unescary information due to other columns.\n",
    "\n",
    "store_and_fwd_flag is used to flag if the vechile had a conection to the service during payment, this information is unnessacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.drop(\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improvement_surcharge, mta_tax, extra, tolls_amount, ehail_fee and congestion_surcharge are all various surcharges applied to the total fee to the customer. These on their own dont hold alot of information but the total will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.na.fill(value=0,subset=[\"congestion_surcharge\", \"ehail_fee\"])\n",
    "sdf = sdf.withColumn('surcharge_amount', col(\"improvement_surcharge\") + col(\"mta_tax\") + col(\"extra\") + col(\"tolls_amount\") + col(\"ehail_fee\") + col(\"congestion_surcharge\"))\n",
    "sdf = sdf.drop(\"extra\", \"mta_tax\", \"improvement_surcharge\", \"congestion_surcharge\", \"tolls_amount\", \"ehail_fee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_time\")\n",
    "sdf = sdf.withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_time\")\n",
    "sdf = sdf.withColumnRenamed(\"PULocationID\", \"pickup_location\")\n",
    "sdf = sdf.withColumnRenamed(\"DOLocationID\", \"dropoff_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trip_distance is set to miles (imperial), thus for ease of analysis, this will be set to kilometres (metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"trip_distance_km\", round(col(\"trip_distance\") * 1.60934, 3)) \n",
    "sdf = sdf.drop('trip_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Trip duration (in minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are time stamps of the pickup an dropoff times, it may be handy to look into aspects affecting the duration of the trip, thus we will add a column for total duration in minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.withColumn('trip_time_sec',unix_timestamp(\"dropoff_time\") - unix_timestamp('pickup_time'))\n",
    "sdf = sdf.withColumn('trip_time_min', round(col('trip_time_sec') / 60, 3))\n",
    "sdf = sdf.drop('trip_time_sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- dropoff_time: timestamp (nullable = true)\n",
      " |-- pickup_location: double (nullable = true)\n",
      " |-- dropoff_location: string (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- surcharge_amount: double (nullable = true)\n",
      " |-- trip_distance_km: double (nullable = true)\n",
      " |-- trip_time_min: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('passenger_count', sdf.passenger_count.cast('int'))\n",
    "sdf = sdf.withColumn('pickup_location', sdf.pickup_location.cast('long'))\n",
    "sdf = sdf.withColumn('dropoff_location', sdf.pickup_location.cast('long'))\n",
    "sdf = sdf.withColumn('payment_type', sdf.payment_type.cast('int'))\n",
    "sdf = sdf.withColumn('payment_type', sdf.payment_type.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalidating data\n",
    "#### passenger_count:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the reasonable minimum nuber of passengers is 1, For taxis in NYC the maximum legal number of passengers is 5, thus we will onyl keep instances between 1 and 5 passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.passenger_count > 0)\n",
    "sdf = sdf.filter(sdf.passenger_count < 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickup/dropoff time:\n",
    "ensuring all data comes from the two year window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.pickup_time >= '2018-01-01 00:00:00')\n",
    "sdf = sdf.filter(sdf.pickup_time < '2020-01-01 00:00:00')\n",
    "sdf = sdf.filter(sdf.dropoff_time >= '2018-01-01 00:00:00')\n",
    "sdf = sdf.filter(sdf.dropoff_time < '2020-01-01 00:00:00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip_distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As New York City from north to south (Tottenville to Wakefield) is approximately 75km long trip according to https://www.taxi-calculator.com/taxi-fare-estimation, we will set that as the upper bound of acceptable taxi trip distances, and of course all non-negative values as the lower bound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.trip_distance_km > 0)\n",
    "sdf = sdf.filter(sdf.trip_distance_km < 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total_amount:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://www.taxifarefinder.com/main.php?city=NY&from=Conference+House+Park%2C+7455+Hylan+Blvd%2C+New+York+City%2C+New+York%2C+10307%2C+United+States+of+America&to=Metro+North+-+Wakefield+Station%2C+E+241st+St%2FWakefield+Ave%2C+New+York+City%2C+New+York%2C+10470%2C+United+States+of+America&fromCoord=40.502859,-74.252418&toCoord=40.905372,-73.85469 the longest trip would cost $129.72 to travel excluding tips, thus assuming a very generous tip of $100, we will set our upper most boundary at $230."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.total_amount > 0)\n",
    "sdf = sdf.filter(sdf.total_amount < 230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip_time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the reasonable maximum travel diatance of 75km, we assume an average travel speed of 40km/h (condering extremely bad traffic as well as highways) which works out to be approximately 2 hours, thus we can set this as the reasonable upper bound and 0 as the reasonable lower bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.trip_time_min > 0)\n",
    "sdf = sdf.filter(sdf.trip_time_min < 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payment Type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data dictionary, only payment types of 1 (credit card) has records of non zero tips thus we will only keep payments type 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.payment_type == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 15:24:02 WARN DAGScheduler: Broadcasting large task binary with size 1159.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geo_tips = sdf[['pickup_location', 'tip_amount']] \\\n",
    "                .groupby('pickup_location') \\\n",
    "                .agg(\n",
    "                    {\n",
    "                        'tip_amount': 'sum', # sum over total amount earned\n",
    "                        'pickup_location': 'count' # count number of instances from sample\n",
    "                    }\n",
    "                )\n",
    "geo_tips = geo_tips.withColumn('avg_tip_amount', col('sum(tip_amount)') / col('count(pickup_location)'))\n",
    "geo_tips = geo_tips.withColumnRenamed('count(pickup_location)', 'total_trips').withColumnRenamed('sum(tip_amount)', 'total_tips')\n",
    "\n",
    "geo_tips.write.parquet(\"DataFrames/geo_tip_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "863906e9a4165d4738736793daf13ef61aad6cfe5622e28d202ed76ed0afc263"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
